{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11660187,"sourceType":"datasetVersion","datasetId":7317398}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import ViTModel, AutoImageProcessor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. CONFIG\nDATA_DIR = \"/kaggle/input/fruit-detection-yolo/fruit-detection-dataset/fruit-detection-dataset\"  # ✏️ adjust if needed\nTRAIN_IMG_DIR = os.path.join(DATA_DIR, \"images/train\")\nTRAIN_LABEL_DIR = os.path.join(DATA_DIR, \"labels/train\")\nVAL_IMG_DIR   = os.path.join(DATA_DIR, \"images/val\")\nVAL_LABEL_DIR = os.path.join(DATA_DIR, \"labels/val\")\nBATCH_SIZE = 16\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nCLASS_NAMES = [\"apple\",\"avocado\",\"banana\",\"guava\",\"kiwi\",\"mango\",\"orange\",\"peach\",\"pineapple\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T00:12:03.932529Z","iopub.execute_input":"2025-06-09T00:12:03.933172Z","iopub.status.idle":"2025-06-09T00:12:03.939023Z","shell.execute_reply.started":"2025-06-09T00:12:03.933148Z","shell.execute_reply":"2025-06-09T00:12:03.938252Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2. BUILD CLASSIFICATION DATAFRAME\ndef build_df(img_dir, label_dir):\n    rows = []\n    for img_path in tqdm(glob.glob(os.path.join(img_dir, \"*\"))):\n        fname = os.path.basename(img_path)\n        lbl_path = os.path.join(label_dir, os.path.splitext(fname)[0] + \".txt\")\n        if not os.path.exists(lbl_path):\n            continue\n        # read first line\n        with open(lbl_path, \"r\") as f:\n            line = f.readline().strip().split()\n        if len(line)==0: continue\n        class_id = int(line[0])\n        rows.append({\"image\": img_path, \"label\": class_id})\n    return pd.DataFrame(rows)\n\ntrain_df = build_df(TRAIN_IMG_DIR, TRAIN_LABEL_DIR)\nval_df   = build_df(VAL_IMG_DIR,   VAL_LABEL_DIR)\n\nprint(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n\n# 3. DATASET + DATALOADER\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-large-patch16-224-in21k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T00:12:06.090907Z","iopub.execute_input":"2025-06-09T00:12:06.091538Z","iopub.status.idle":"2025-06-09T00:12:06.937625Z","shell.execute_reply.started":"2025-06-09T00:12:06.091514Z","shell.execute_reply":"2025-06-09T00:12:06.937027Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/534 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1195b1dc9e04423beddfeb28844ddbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/134 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c0fde844404a72a6d35097f439ad01"}},"metadata":{}},{"name":"stderr","text":"Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","output_type":"stream"},{"name":"stdout","text":"Train samples: 534, Val samples: 134\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class FruitClsDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img = Image.open(row.image).convert(\"RGB\")\n        # HF processor returns pixel_values tensor\n        pv = self.processor(images=img, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        label = row.label\n        return pv, label\n\ndef collate_fn(batch):\n    imgs = torch.stack([b[0] for b in batch], dim=0)\n    labels = torch.tensor([b[1] for b in batch], dtype=torch.long)\n    return imgs, labels\n\n# extract embeddings function\ndef extract_embeddings(df):\n    ds = FruitClsDataset(df, processor)\n    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    model = ViTModel.from_pretrained(\"google/vit-large-patch16-224-in21k\").eval().to(DEVICE)\n    all_embs = []\n    all_labels = []\n    with torch.no_grad():\n        for imgs, labels in tqdm(dl):\n            imgs = imgs.to(DEVICE)\n            out = model(imgs).last_hidden_state[:,0]   # take CLS token\n            all_embs.append(out.cpu().numpy())\n            all_labels.append(labels.numpy())\n    embs = np.concatenate(all_embs, axis=0)\n    labs =  np.concatenate(all_labels,axis=0)\n    return embs, labs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T00:12:10.991896Z","iopub.execute_input":"2025-06-09T00:12:10.992575Z","iopub.status.idle":"2025-06-09T00:12:10.999403Z","shell.execute_reply.started":"2025-06-09T00:12:10.992550Z","shell.execute_reply":"2025-06-09T00:12:10.998841Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 4. EXTRACT FEATURES\nprint(\"Extracting train embeddings…\")\nX_train, y_train = extract_embeddings(train_df)\nprint(\"Extracting val embeddings…\")\nX_val,   y_val   = extract_embeddings(val_df)\n\n# 5. TRAIN & EVALUATE CLASSIFIERS\nclassifiers = {\n    \"LogisticRegression\": LogisticRegression(max_iter=200, n_jobs=-1),\n    \"SVM (RBF)\":          SVC(gamma=\"scale\"),\n    \"RandomForest\":       RandomForestClassifier(n_estimators=100, n_jobs=-1),\n    \"GradientBoosting\":   GradientBoostingClassifier(n_estimators=100),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T00:12:22.841432Z","iopub.execute_input":"2025-06-09T00:12:22.841694Z","iopub.status.idle":"2025-06-09T00:12:44.888060Z","shell.execute_reply.started":"2025-06-09T00:12:22.841676Z","shell.execute_reply":"2025-06-09T00:12:44.887376Z"}},"outputs":[{"name":"stdout","text":"Extracting train embeddings…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c2175a2c934b1ca3648f4aae799c70"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Extracting val embeddings…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb373985d6346f68e7d9083e31195d0"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"results = []\nfor name, clf in classifiers.items():\n    print(f\"\\nTraining {name}…\")\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_val)\n    acc   = accuracy_score(y_val, preds)\n    print(f\"→ {name} validation accuracy: {acc:.4f}\")\n    print(classification_report(y_val, preds, target_names=CLASS_NAMES, zero_division=0))\n    results.append({\"model\": name, \"val_acc\": acc})\n\n# 6. SUMMARY\nres_df = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False)\nprint(\"\\n=== Summary ===\")\nprint(res_df)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T00:07:10.373934Z","iopub.execute_input":"2025-06-09T00:07:10.374596Z","iopub.status.idle":"2025-06-09T00:09:59.857487Z","shell.execute_reply.started":"2025-06-09T00:07:10.374572Z","shell.execute_reply":"2025-06-09T00:09:59.856466Z"}},"outputs":[{"name":"stdout","text":"Extracting train embeddings…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a477ddc8aba4e4487cca5bcaedaffe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25bc599b11b34946a0289dafebe397d4"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Extracting val embeddings…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9f4b296a24a48f793a8baab5842e141"}},"metadata":{}},{"name":"stdout","text":"\nTraining LogisticRegression…\n→ LogisticRegression validation accuracy: 0.9627\n              precision    recall  f1-score   support\n\n       apple       1.00      0.91      0.95        22\n     avocado       1.00      1.00      1.00        11\n      banana       0.95      1.00      0.97        18\n       guava       1.00      1.00      1.00         9\n        kiwi       1.00      1.00      1.00        13\n       mango       1.00      0.91      0.95        11\n      orange       0.90      0.95      0.92        19\n       peach       0.89      1.00      0.94        17\n   pineapple       1.00      0.93      0.96        14\n\n    accuracy                           0.96       134\n   macro avg       0.97      0.97      0.97       134\nweighted avg       0.97      0.96      0.96       134\n\n\nTraining SVM (RBF)…\n→ SVM (RBF) validation accuracy: 0.9403\n              precision    recall  f1-score   support\n\n       apple       1.00      0.86      0.93        22\n     avocado       1.00      0.82      0.90        11\n      banana       0.86      1.00      0.92        18\n       guava       1.00      1.00      1.00         9\n        kiwi       1.00      1.00      1.00        13\n       mango       1.00      0.91      0.95        11\n      orange       0.78      0.95      0.86        19\n       peach       1.00      1.00      1.00        17\n   pineapple       1.00      0.93      0.96        14\n\n    accuracy                           0.94       134\n   macro avg       0.96      0.94      0.95       134\nweighted avg       0.95      0.94      0.94       134\n\n\nTraining RandomForest…\n→ RandomForest validation accuracy: 0.9403\n              precision    recall  f1-score   support\n\n       apple       0.95      0.86      0.90        22\n     avocado       1.00      0.82      0.90        11\n      banana       0.86      1.00      0.92        18\n       guava       0.90      1.00      0.95         9\n        kiwi       1.00      1.00      1.00        13\n       mango       1.00      0.91      0.95        11\n      orange       0.90      0.95      0.92        19\n       peach       0.94      1.00      0.97        17\n   pineapple       1.00      0.93      0.96        14\n\n    accuracy                           0.94       134\n   macro avg       0.95      0.94      0.94       134\nweighted avg       0.94      0.94      0.94       134\n\n\nTraining GradientBoosting…\n→ GradientBoosting validation accuracy: 0.9254\n              precision    recall  f1-score   support\n\n       apple       0.86      0.86      0.86        22\n     avocado       1.00      0.82      0.90        11\n      banana       0.85      0.94      0.89        18\n       guava       0.82      1.00      0.90         9\n        kiwi       1.00      1.00      1.00        13\n       mango       1.00      0.82      0.90        11\n      orange       0.95      0.95      0.95        19\n       peach       1.00      0.94      0.97        17\n   pineapple       0.93      1.00      0.97        14\n\n    accuracy                           0.93       134\n   macro avg       0.93      0.93      0.93       134\nweighted avg       0.93      0.93      0.93       134\n\n\n=== Summary ===\n                model   val_acc\n0  LogisticRegression  0.962687\n1           SVM (RBF)  0.940299\n2        RandomForest  0.940299\n3    GradientBoosting  0.925373\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}